{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]/255\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "y_test_num = le.fit_transform(y_test)\n",
    "y_train_num = le.fit_transform(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9', '2', '1', ..., '0', '8', '2'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 7, 4, ..., 6, 5, 9])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP = MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(20, 50, 70, 100, 70, 50, 20),\n",
    "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=500,\n",
    "       momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "       power_t=0.5, random_state=None, shuffle=True, solver='sgd',\n",
    "       tol=0.0001, validation_fraction=0.1, verbose=True,\n",
    "       warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.35629481\n",
      "Iteration 2, loss = 2.27114907\n",
      "Iteration 3, loss = 2.03276656\n",
      "Iteration 4, loss = 1.56340004\n",
      "Iteration 5, loss = 0.94788936\n",
      "Iteration 6, loss = 0.68073956\n",
      "Iteration 7, loss = 0.54478800\n",
      "Iteration 8, loss = 0.46709676\n",
      "Iteration 9, loss = 0.41806456\n",
      "Iteration 10, loss = 0.38368878\n",
      "Iteration 11, loss = 0.35614617\n",
      "Iteration 12, loss = 0.33555402\n",
      "Iteration 13, loss = 0.31854617\n",
      "Iteration 14, loss = 0.30475627\n",
      "Iteration 15, loss = 0.29361992\n",
      "Iteration 16, loss = 0.28439550\n",
      "Iteration 17, loss = 0.27571947\n",
      "Iteration 18, loss = 0.26848996\n",
      "Iteration 19, loss = 0.26166863\n",
      "Iteration 20, loss = 0.25442274\n",
      "Iteration 21, loss = 0.24898442\n",
      "Iteration 22, loss = 0.24435561\n",
      "Iteration 23, loss = 0.23911001\n",
      "Iteration 24, loss = 0.23508264\n",
      "Iteration 25, loss = 0.23001286\n",
      "Iteration 26, loss = 0.22702330\n",
      "Iteration 27, loss = 0.22313684\n",
      "Iteration 28, loss = 0.21909396\n",
      "Iteration 29, loss = 0.21503175\n",
      "Iteration 30, loss = 0.21247397\n",
      "Iteration 31, loss = 0.20907775\n",
      "Iteration 32, loss = 0.20599990\n",
      "Iteration 33, loss = 0.20336760\n",
      "Iteration 34, loss = 0.20079584\n",
      "Iteration 35, loss = 0.19817786\n",
      "Iteration 36, loss = 0.19567664\n",
      "Iteration 37, loss = 0.19321877\n",
      "Iteration 38, loss = 0.19109958\n",
      "Iteration 39, loss = 0.18867906\n",
      "Iteration 40, loss = 0.18621581\n",
      "Iteration 41, loss = 0.18515141\n",
      "Iteration 42, loss = 0.18254144\n",
      "Iteration 43, loss = 0.18087303\n",
      "Iteration 44, loss = 0.17866376\n",
      "Iteration 45, loss = 0.17732634\n",
      "Iteration 46, loss = 0.17529374\n",
      "Iteration 47, loss = 0.17390871\n",
      "Iteration 48, loss = 0.17213492\n",
      "Iteration 49, loss = 0.16997707\n",
      "Iteration 50, loss = 0.16969831\n",
      "Iteration 51, loss = 0.16781561\n",
      "Iteration 52, loss = 0.16609980\n",
      "Iteration 53, loss = 0.16495819\n",
      "Iteration 54, loss = 0.16300578\n",
      "Iteration 55, loss = 0.16142850\n",
      "Iteration 56, loss = 0.16090959\n",
      "Iteration 57, loss = 0.15937688\n",
      "Iteration 58, loss = 0.15862965\n",
      "Iteration 59, loss = 0.15677327\n",
      "Iteration 60, loss = 0.15610693\n",
      "Iteration 61, loss = 0.15479602\n",
      "Iteration 62, loss = 0.15372628\n",
      "Iteration 63, loss = 0.15234925\n",
      "Iteration 64, loss = 0.15072872\n",
      "Iteration 65, loss = 0.15006485\n",
      "Iteration 66, loss = 0.14897177\n",
      "Iteration 67, loss = 0.14804942\n",
      "Iteration 68, loss = 0.14703985\n",
      "Iteration 69, loss = 0.14589164\n",
      "Iteration 70, loss = 0.14529177\n",
      "Iteration 71, loss = 0.14485632\n",
      "Iteration 72, loss = 0.14283448\n",
      "Iteration 73, loss = 0.14229924\n",
      "Iteration 74, loss = 0.14174741\n",
      "Iteration 75, loss = 0.14074509\n",
      "Iteration 76, loss = 0.13937655\n",
      "Iteration 77, loss = 0.13889325\n",
      "Iteration 78, loss = 0.13805748\n",
      "Iteration 79, loss = 0.13665438\n",
      "Iteration 80, loss = 0.13621909\n",
      "Iteration 81, loss = 0.13495440\n",
      "Iteration 82, loss = 0.13411402\n",
      "Iteration 83, loss = 0.13332265\n",
      "Iteration 84, loss = 0.13304266\n",
      "Iteration 85, loss = 0.13200221\n",
      "Iteration 86, loss = 0.13091847\n",
      "Iteration 87, loss = 0.13018266\n",
      "Iteration 88, loss = 0.12988815\n",
      "Iteration 89, loss = 0.12771622\n",
      "Iteration 90, loss = 0.12778053\n",
      "Iteration 91, loss = 0.12733575\n",
      "Iteration 92, loss = 0.12663102\n",
      "Iteration 93, loss = 0.12531367\n",
      "Iteration 94, loss = 0.12497490\n",
      "Iteration 95, loss = 0.12392004\n",
      "Iteration 96, loss = 0.12343741\n",
      "Iteration 97, loss = 0.12267959\n",
      "Iteration 98, loss = 0.12186756\n",
      "Iteration 99, loss = 0.12130267\n",
      "Iteration 100, loss = 0.12042658\n",
      "Iteration 101, loss = 0.12010388\n",
      "Iteration 102, loss = 0.11933391\n",
      "Iteration 103, loss = 0.11863446\n",
      "Iteration 104, loss = 0.11783956\n",
      "Iteration 105, loss = 0.11758183\n",
      "Iteration 106, loss = 0.11669325\n",
      "Iteration 107, loss = 0.11573337\n",
      "Iteration 108, loss = 0.11574023\n",
      "Iteration 109, loss = 0.11508528\n",
      "Iteration 110, loss = 0.11379825\n",
      "Iteration 111, loss = 0.11371281\n",
      "Iteration 112, loss = 0.11269105\n",
      "Iteration 113, loss = 0.11261291\n",
      "Iteration 114, loss = 0.11203230\n",
      "Iteration 115, loss = 0.11181501\n",
      "Iteration 116, loss = 0.11082382\n",
      "Iteration 117, loss = 0.11030212\n",
      "Iteration 118, loss = 0.10975775\n",
      "Iteration 119, loss = 0.10889946\n",
      "Iteration 120, loss = 0.10805398\n",
      "Iteration 121, loss = 0.10768782\n",
      "Iteration 122, loss = 0.10724169\n",
      "Iteration 123, loss = 0.10707725\n",
      "Iteration 124, loss = 0.10632725\n",
      "Iteration 125, loss = 0.10603415\n",
      "Iteration 126, loss = 0.10528566\n",
      "Iteration 127, loss = 0.10461522\n",
      "Iteration 128, loss = 0.10434042\n",
      "Iteration 129, loss = 0.10427668\n",
      "Iteration 130, loss = 0.10318526\n",
      "Iteration 131, loss = 0.10275112\n",
      "Iteration 132, loss = 0.10177016\n",
      "Iteration 133, loss = 0.10147201\n",
      "Iteration 134, loss = 0.10130567\n",
      "Iteration 135, loss = 0.10129150\n",
      "Iteration 136, loss = 0.10034544\n",
      "Iteration 137, loss = 0.09977656\n",
      "Iteration 138, loss = 0.09948306\n",
      "Iteration 139, loss = 0.09886516\n",
      "Iteration 140, loss = 0.09860809\n",
      "Iteration 141, loss = 0.09774238\n",
      "Iteration 142, loss = 0.09736398\n",
      "Iteration 143, loss = 0.09653590\n",
      "Iteration 144, loss = 0.09660617\n",
      "Iteration 145, loss = 0.09647018\n",
      "Iteration 146, loss = 0.09618583\n",
      "Iteration 147, loss = 0.09545691\n",
      "Iteration 148, loss = 0.09510720\n",
      "Iteration 149, loss = 0.09437890\n",
      "Iteration 150, loss = 0.09442371\n",
      "Iteration 151, loss = 0.09395914\n",
      "Iteration 152, loss = 0.09397261\n",
      "Iteration 153, loss = 0.09350448\n",
      "Iteration 154, loss = 0.09348395\n",
      "Iteration 155, loss = 0.09311726\n",
      "Iteration 156, loss = 0.09169761\n",
      "Iteration 157, loss = 0.09205633\n",
      "Iteration 158, loss = 0.09191761\n",
      "Iteration 159, loss = 0.09063683\n",
      "Iteration 160, loss = 0.09094658\n",
      "Iteration 161, loss = 0.09061937\n",
      "Iteration 162, loss = 0.08993330\n",
      "Iteration 163, loss = 0.08906252\n",
      "Iteration 164, loss = 0.08946983\n",
      "Iteration 165, loss = 0.08893708\n",
      "Iteration 166, loss = 0.08818861\n",
      "Iteration 167, loss = 0.08807078\n",
      "Iteration 168, loss = 0.08793539\n",
      "Iteration 169, loss = 0.08731448\n",
      "Iteration 170, loss = 0.08687324\n",
      "Iteration 171, loss = 0.08715443\n",
      "Iteration 172, loss = 0.08655694\n",
      "Iteration 173, loss = 0.08656636\n",
      "Iteration 174, loss = 0.08503201\n",
      "Iteration 175, loss = 0.08514504\n",
      "Iteration 176, loss = 0.08535679\n",
      "Iteration 177, loss = 0.08533527\n",
      "Iteration 178, loss = 0.08415124\n",
      "Iteration 179, loss = 0.08376468\n",
      "Iteration 180, loss = 0.08407541\n",
      "Iteration 181, loss = 0.08303571\n",
      "Iteration 182, loss = 0.08323145\n",
      "Iteration 183, loss = 0.08341768\n",
      "Iteration 184, loss = 0.08312712\n",
      "Iteration 185, loss = 0.08240671\n",
      "Iteration 186, loss = 0.08174344\n",
      "Iteration 187, loss = 0.08172649\n",
      "Iteration 188, loss = 0.08147926\n",
      "Iteration 189, loss = 0.08111324\n",
      "Iteration 190, loss = 0.08097216\n",
      "Iteration 191, loss = 0.08218523\n",
      "Iteration 192, loss = 0.08086117\n",
      "Iteration 193, loss = 0.07988653\n",
      "Iteration 194, loss = 0.08040619\n",
      "Iteration 195, loss = 0.07977057\n",
      "Iteration 196, loss = 0.07923354\n",
      "Iteration 197, loss = 0.07890403\n",
      "Iteration 198, loss = 0.07861637\n",
      "Iteration 199, loss = 0.07851464\n",
      "Iteration 200, loss = 0.07839468\n",
      "Iteration 201, loss = 0.07821370\n",
      "Iteration 202, loss = 0.07840564\n",
      "Iteration 203, loss = 0.07836785\n",
      "Iteration 204, loss = 0.07737859\n",
      "Iteration 205, loss = 0.07715455\n",
      "Iteration 206, loss = 0.07722191\n",
      "Iteration 207, loss = 0.07636953\n",
      "Iteration 208, loss = 0.07714833\n",
      "Iteration 209, loss = 0.07603228\n",
      "Iteration 210, loss = 0.07628238\n",
      "Iteration 211, loss = 0.07590051\n",
      "Iteration 212, loss = 0.07541398\n",
      "Iteration 213, loss = 0.07500675\n",
      "Iteration 214, loss = 0.07519067\n",
      "Iteration 215, loss = 0.07451435\n",
      "Iteration 216, loss = 0.07452914\n",
      "Iteration 217, loss = 0.07440216\n",
      "Iteration 218, loss = 0.07397771\n",
      "Iteration 219, loss = 0.07425872\n",
      "Iteration 220, loss = 0.07367445\n",
      "Iteration 221, loss = 0.07345835\n",
      "Iteration 222, loss = 0.07353719\n",
      "Iteration 223, loss = 0.07332824\n",
      "Iteration 224, loss = 0.07285375\n",
      "Iteration 225, loss = 0.07282969\n",
      "Iteration 226, loss = 0.07227655\n",
      "Iteration 227, loss = 0.07249310\n",
      "Iteration 228, loss = 0.07204209\n",
      "Iteration 229, loss = 0.07189925\n",
      "Iteration 230, loss = 0.07114776\n",
      "Iteration 231, loss = 0.07168166\n",
      "Iteration 232, loss = 0.07090329\n",
      "Iteration 233, loss = 0.07143626\n",
      "Iteration 234, loss = 0.07146353\n",
      "Iteration 235, loss = 0.07062312\n",
      "Iteration 236, loss = 0.07002362\n",
      "Iteration 237, loss = 0.06999613\n",
      "Iteration 238, loss = 0.07034291\n",
      "Iteration 239, loss = 0.07042430\n",
      "Iteration 240, loss = 0.07013188\n",
      "Iteration 241, loss = 0.06942728\n",
      "Iteration 242, loss = 0.06935474\n",
      "Iteration 243, loss = 0.06942374\n",
      "Iteration 244, loss = 0.06950356\n",
      "Iteration 245, loss = 0.06897064\n",
      "Iteration 246, loss = 0.06894113\n",
      "Iteration 247, loss = 0.06871878\n",
      "Iteration 248, loss = 0.06923201\n",
      "Iteration 249, loss = 0.06837070\n",
      "Iteration 250, loss = 0.06772621\n",
      "Iteration 251, loss = 0.06838531\n",
      "Iteration 252, loss = 0.06788168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.06784429\n",
      "Iteration 254, loss = 0.06755548\n",
      "Iteration 255, loss = 0.06707553\n",
      "Iteration 256, loss = 0.06679249\n",
      "Iteration 257, loss = 0.06689036\n",
      "Iteration 258, loss = 0.06701725\n",
      "Iteration 259, loss = 0.06715781\n",
      "Iteration 260, loss = 0.06642436\n",
      "Iteration 261, loss = 0.06626521\n",
      "Iteration 262, loss = 0.06668951\n",
      "Iteration 263, loss = 0.06662453\n",
      "Iteration 264, loss = 0.06591989\n",
      "Iteration 265, loss = 0.06609414\n",
      "Iteration 266, loss = 0.06591334\n",
      "Iteration 267, loss = 0.06531544\n",
      "Iteration 268, loss = 0.06529282\n",
      "Iteration 269, loss = 0.06568497\n",
      "Iteration 270, loss = 0.06552278\n",
      "Iteration 271, loss = 0.06531234\n",
      "Iteration 272, loss = 0.06468431\n",
      "Iteration 273, loss = 0.06472928\n",
      "Iteration 274, loss = 0.06499747\n",
      "Iteration 275, loss = 0.06507446\n",
      "Iteration 276, loss = 0.06513086\n",
      "Iteration 277, loss = 0.06403999\n",
      "Iteration 278, loss = 0.06414760\n",
      "Iteration 279, loss = 0.06386221\n",
      "Iteration 280, loss = 0.06433494\n",
      "Iteration 281, loss = 0.06362486\n",
      "Iteration 282, loss = 0.06340215\n",
      "Iteration 283, loss = 0.06396968\n",
      "Iteration 284, loss = 0.06353802\n",
      "Iteration 285, loss = 0.06328265\n",
      "Iteration 286, loss = 0.06335357\n",
      "Iteration 287, loss = 0.06297748\n",
      "Iteration 288, loss = 0.06287086\n",
      "Iteration 289, loss = 0.06269451\n",
      "Iteration 290, loss = 0.06231687\n",
      "Iteration 291, loss = 0.06317432\n",
      "Iteration 292, loss = 0.06227594\n",
      "Iteration 293, loss = 0.06212745\n",
      "Iteration 294, loss = 0.06239639\n",
      "Iteration 295, loss = 0.06200692\n",
      "Iteration 296, loss = 0.06212379\n",
      "Iteration 297, loss = 0.06191188\n",
      "Iteration 298, loss = 0.06146436\n",
      "Iteration 299, loss = 0.06150827\n",
      "Iteration 300, loss = 0.06136050\n",
      "Iteration 301, loss = 0.06147370\n",
      "Iteration 302, loss = 0.06137004\n",
      "Iteration 303, loss = 0.06142999\n",
      "Iteration 304, loss = 0.06104281\n",
      "Iteration 305, loss = 0.06100727\n",
      "Iteration 306, loss = 0.06082342\n",
      "Iteration 307, loss = 0.06117920\n",
      "Iteration 308, loss = 0.06071215\n",
      "Iteration 309, loss = 0.06076073\n",
      "Iteration 310, loss = 0.06041349\n",
      "Iteration 311, loss = 0.06022220\n",
      "Iteration 312, loss = 0.06093534\n",
      "Iteration 313, loss = 0.05999055\n",
      "Iteration 314, loss = 0.06008709\n",
      "Iteration 315, loss = 0.06045049\n",
      "Iteration 316, loss = 0.05991999\n",
      "Iteration 317, loss = 0.05997224\n",
      "Iteration 318, loss = 0.05951352\n",
      "Iteration 319, loss = 0.05949266\n",
      "Iteration 320, loss = 0.05949853\n",
      "Iteration 321, loss = 0.05950665\n",
      "Iteration 322, loss = 0.05999539\n",
      "Iteration 323, loss = 0.05943769\n",
      "Iteration 324, loss = 0.05904579\n",
      "Iteration 325, loss = 0.05993009\n",
      "Iteration 326, loss = 0.05899628\n",
      "Iteration 327, loss = 0.05921227\n",
      "Iteration 328, loss = 0.05871722\n",
      "Iteration 329, loss = 0.05913750\n",
      "Iteration 330, loss = 0.05873290\n",
      "Iteration 331, loss = 0.05889619\n",
      "Iteration 332, loss = 0.05871714\n",
      "Iteration 333, loss = 0.05860862\n",
      "Iteration 334, loss = 0.05844836\n",
      "Iteration 335, loss = 0.05848380\n",
      "Iteration 336, loss = 0.05855050\n",
      "Iteration 337, loss = 0.05867251\n",
      "Iteration 338, loss = 0.05828692\n",
      "Iteration 339, loss = 0.05832588\n",
      "Iteration 340, loss = 0.05837626\n",
      "Iteration 341, loss = 0.05798617\n",
      "Iteration 342, loss = 0.05823814\n",
      "Iteration 343, loss = 0.05793690\n",
      "Iteration 344, loss = 0.05773920\n",
      "Iteration 345, loss = 0.05745837\n",
      "Iteration 346, loss = 0.05765831\n",
      "Iteration 347, loss = 0.05753569\n",
      "Iteration 348, loss = 0.05751495\n",
      "Iteration 349, loss = 0.05747905\n",
      "Iteration 350, loss = 0.05734376\n",
      "Iteration 351, loss = 0.05728261\n",
      "Iteration 352, loss = 0.05715653\n",
      "Iteration 353, loss = 0.05721097\n",
      "Iteration 354, loss = 0.05712467\n",
      "Iteration 355, loss = 0.05731799\n",
      "Iteration 356, loss = 0.05680293\n",
      "Iteration 357, loss = 0.05818671\n",
      "Iteration 358, loss = 0.05683316\n",
      "Iteration 359, loss = 0.05679481\n",
      "Iteration 360, loss = 0.05681346\n",
      "Iteration 361, loss = 0.05681571\n",
      "Iteration 362, loss = 0.05679742\n",
      "Iteration 363, loss = 0.05639727\n",
      "Iteration 364, loss = 0.05635208\n",
      "Iteration 365, loss = 0.05632446\n",
      "Iteration 366, loss = 0.05652576\n",
      "Iteration 367, loss = 0.05631177\n",
      "Iteration 368, loss = 0.05610935\n",
      "Iteration 369, loss = 0.05609781\n",
      "Iteration 370, loss = 0.05611656\n",
      "Iteration 371, loss = 0.05602037\n",
      "Iteration 372, loss = 0.05615133\n",
      "Iteration 373, loss = 0.05593789\n",
      "Iteration 374, loss = 0.05594509\n",
      "Iteration 375, loss = 0.05628640\n",
      "Iteration 376, loss = 0.05582146\n",
      "Iteration 377, loss = 0.05552570\n",
      "Iteration 378, loss = 0.05571812\n",
      "Iteration 379, loss = 0.05563319\n",
      "Iteration 380, loss = 0.05561368\n",
      "Iteration 381, loss = 0.05541660\n",
      "Iteration 382, loss = 0.05546767\n",
      "Iteration 383, loss = 0.05540440\n",
      "Iteration 384, loss = 0.05529886\n",
      "Iteration 385, loss = 0.05530980\n",
      "Iteration 386, loss = 0.05535933\n",
      "Iteration 387, loss = 0.05526179\n",
      "Iteration 388, loss = 0.05514108\n",
      "Iteration 389, loss = 0.05507657\n",
      "Iteration 390, loss = 0.05511233\n",
      "Iteration 391, loss = 0.05498741\n",
      "Iteration 392, loss = 0.05497768\n",
      "Iteration 393, loss = 0.05506761\n",
      "Iteration 394, loss = 0.05485331\n",
      "Iteration 395, loss = 0.05492336\n",
      "Iteration 396, loss = 0.05484293\n",
      "Iteration 397, loss = 0.05490267\n",
      "Iteration 398, loss = 0.05465752\n",
      "Iteration 399, loss = 0.05467929\n",
      "Iteration 400, loss = 0.05473678\n",
      "Iteration 401, loss = 0.05462097\n",
      "Iteration 402, loss = 0.05451166\n",
      "Iteration 403, loss = 0.05449989\n",
      "Iteration 404, loss = 0.05468725\n",
      "Iteration 405, loss = 0.05443020\n",
      "Iteration 406, loss = 0.05433920\n",
      "Iteration 407, loss = 0.05437909\n",
      "Iteration 408, loss = 0.05434182\n",
      "Iteration 409, loss = 0.05426210\n",
      "Iteration 410, loss = 0.05420322\n",
      "Iteration 411, loss = 0.05424378\n",
      "Iteration 412, loss = 0.05427456\n",
      "Iteration 413, loss = 0.05400876\n",
      "Iteration 414, loss = 0.05411984\n",
      "Iteration 415, loss = 0.05398633\n",
      "Iteration 416, loss = 0.05389182\n",
      "Iteration 417, loss = 0.05395982\n",
      "Iteration 418, loss = 0.05384455\n",
      "Iteration 419, loss = 0.05380074\n",
      "Iteration 420, loss = 0.05381836\n",
      "Iteration 421, loss = 0.05372949\n",
      "Iteration 422, loss = 0.05371100\n",
      "Iteration 423, loss = 0.05365791\n",
      "Iteration 424, loss = 0.05373703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000200\n",
      "Iteration 425, loss = 0.05332487\n",
      "Iteration 426, loss = 0.05322228\n",
      "Iteration 427, loss = 0.05319678\n",
      "Iteration 428, loss = 0.05318301\n",
      "Iteration 429, loss = 0.05317963\n",
      "Iteration 430, loss = 0.05317649\n",
      "Iteration 431, loss = 0.05315477\n",
      "Iteration 432, loss = 0.05314687\n",
      "Iteration 433, loss = 0.05314930\n",
      "Iteration 434, loss = 0.05313549\n",
      "Iteration 435, loss = 0.05312647\n",
      "Iteration 436, loss = 0.05311279\n",
      "Iteration 437, loss = 0.05310622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000040\n",
      "Iteration 438, loss = 0.05305639\n",
      "Iteration 439, loss = 0.05304824\n",
      "Iteration 440, loss = 0.05304661\n",
      "Iteration 441, loss = 0.05304560\n",
      "Iteration 442, loss = 0.05304427\n",
      "Iteration 443, loss = 0.05304003\n",
      "Iteration 444, loss = 0.05304064\n",
      "Iteration 445, loss = 0.05303916\n",
      "Iteration 446, loss = 0.05303824\n",
      "Iteration 447, loss = 0.05303425\n",
      "Iteration 448, loss = 0.05303612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000008\n",
      "Iteration 449, loss = 0.05302194\n",
      "Iteration 450, loss = 0.05302124\n",
      "Iteration 451, loss = 0.05302085\n",
      "Iteration 452, loss = 0.05302095\n",
      "Iteration 453, loss = 0.05302040\n",
      "Iteration 454, loss = 0.05301986\n",
      "Iteration 455, loss = 0.05301967\n",
      "Iteration 456, loss = 0.05301954\n",
      "Iteration 457, loss = 0.05301883\n",
      "Iteration 458, loss = 0.05301842\n",
      "Iteration 459, loss = 0.05301839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000002\n",
      "Iteration 460, loss = 0.05301573\n",
      "Iteration 461, loss = 0.05301564\n",
      "Iteration 462, loss = 0.05301547\n",
      "Iteration 463, loss = 0.05301545\n",
      "Iteration 464, loss = 0.05301532\n",
      "Iteration 465, loss = 0.05301521\n",
      "Iteration 466, loss = 0.05301515\n",
      "Iteration 467, loss = 0.05301514\n",
      "Iteration 468, loss = 0.05301514\n",
      "Iteration 469, loss = 0.05301501\n",
      "Iteration 470, loss = 0.05301497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 471, loss = 0.05301436\n",
      "Iteration 472, loss = 0.05301435\n",
      "Iteration 473, loss = 0.05301434\n",
      "Iteration 474, loss = 0.05301431\n",
      "Iteration 475, loss = 0.05301430\n",
      "Iteration 476, loss = 0.05301430\n",
      "Iteration 477, loss = 0.05301428\n",
      "Iteration 478, loss = 0.05301427\n",
      "Iteration 479, loss = 0.05301425\n",
      "Iteration 480, loss = 0.05301424\n",
      "Iteration 481, loss = 0.05301422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 50, 70, 100, 70, 50, 20),\n",
       "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=500,\n",
       "       momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "       power_t=0.5, random_state=None, shuffle=True, solver='sgd',\n",
       "       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.fit(X_train,y_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification_report=\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9758    0.9764    0.9761      1694\n",
      "           1     0.9793    0.9773    0.9783      1939\n",
      "           2     0.9501    0.9555    0.9528      1752\n",
      "           3     0.9436    0.9537    0.9486      1771\n",
      "           4     0.9540    0.9513    0.9527      1724\n",
      "           5     0.9478    0.9532    0.9505      1580\n",
      "           6     0.9717    0.9689    0.9703      1769\n",
      "           7     0.9651    0.9656    0.9653      1802\n",
      "           8     0.9471    0.9388    0.9429      1715\n",
      "           9     0.9489    0.9430    0.9460      1754\n",
      "\n",
      "   micro avg     0.9586    0.9586    0.9586     17500\n",
      "   macro avg     0.9583    0.9584    0.9583     17500\n",
      "weighted avg     0.9586    0.9586    0.9586     17500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test_num, y_pred, digits=4)\n",
    "print(f'\\nClassification_report=\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.33761586\n",
      "Iteration 2, loss = 0.16615814\n",
      "Iteration 3, loss = 0.12185264\n",
      "Iteration 4, loss = 0.09837272\n",
      "Iteration 5, loss = 0.08344661\n",
      "Iteration 6, loss = 0.07370062\n",
      "Iteration 7, loss = 0.06427190\n",
      "Iteration 8, loss = 0.05644628\n",
      "Iteration 9, loss = 0.05164080\n",
      "Iteration 10, loss = 0.04431318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\au264346\\AppData\\Local\\Continuum\\anaconda3_2\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50,), learning_rate='constant',\n",
       "       learning_rate_init=0.1, max_iter=10, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(X_train, y_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification_report=\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9822    0.9782    0.9802      1694\n",
      "           1     0.9825    0.9856    0.9840      1939\n",
      "           2     0.9539    0.9795    0.9665      1752\n",
      "           3     0.9702    0.9560    0.9630      1771\n",
      "           4     0.9788    0.9640    0.9714      1724\n",
      "           5     0.9506    0.9741    0.9622      1580\n",
      "           6     0.9813    0.9796    0.9805      1769\n",
      "           7     0.9792    0.9673    0.9732      1802\n",
      "           8     0.9571    0.9633    0.9602      1715\n",
      "           9     0.9689    0.9584    0.9636      1754\n",
      "\n",
      "   micro avg     0.9707    0.9707    0.9707     17500\n",
      "   macro avg     0.9705    0.9706    0.9705     17500\n",
      "weighted avg     0.9708    0.9707    0.9707     17500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test_num, y_pred_mlp, digits=4)\n",
    "print(f'\\nClassification_report=\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 70)                3570      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               7100      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 70)                7070      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                3550      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 39,270\n",
      "Trainable params: 39,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(70, activation='relu'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(70, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "model.summary()\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',  optimizer=sgd,  metrics=['accuracy', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "52500/52500 [==============================] - 4s 84us/step - loss: 0.5523 - acc: 0.8225 - mean_absolute_error: 0.0473\n",
      "Epoch 2/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.2480 - acc: 0.9322 - mean_absolute_error: 0.0205\n",
      "Epoch 3/20\n",
      "52500/52500 [==============================] - 2s 40us/step - loss: 0.2041 - acc: 0.9442 - mean_absolute_error: 0.0170\n",
      "Epoch 4/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.1733 - acc: 0.9514 - mean_absolute_error: 0.0146\n",
      "Epoch 5/20\n",
      "52500/52500 [==============================] - 2s 40us/step - loss: 0.1625 - acc: 0.9547 - mean_absolute_error: 0.0137\n",
      "Epoch 6/20\n",
      "52500/52500 [==============================] - 2s 40us/step - loss: 0.1460 - acc: 0.9597 - mean_absolute_error: 0.0122\n",
      "Epoch 7/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.1380 - acc: 0.9613 - mean_absolute_error: 0.0116\n",
      "Epoch 8/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.1319 - acc: 0.9635 - mean_absolute_error: 0.0111\n",
      "Epoch 9/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.1242 - acc: 0.9644 - mean_absolute_error: 0.0106\n",
      "Epoch 10/20\n",
      "52500/52500 [==============================] - 2s 40us/step - loss: 0.1170 - acc: 0.9670 - mean_absolute_error: 0.0099\n",
      "Epoch 11/20\n",
      "52500/52500 [==============================] - 2s 40us/step - loss: 0.1126 - acc: 0.9682 - mean_absolute_error: 0.0096\n",
      "Epoch 12/20\n",
      "52500/52500 [==============================] - 2s 42us/step - loss: 0.1077 - acc: 0.9694 - mean_absolute_error: 0.0091\n",
      "Epoch 13/20\n",
      "52500/52500 [==============================] - 2s 40us/step - loss: 0.1067 - acc: 0.9695 - mean_absolute_error: 0.0091\n",
      "Epoch 14/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.1006 - acc: 0.9713 - mean_absolute_error: 0.0086\n",
      "Epoch 15/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.0997 - acc: 0.9714 - mean_absolute_error: 0.0084\n",
      "Epoch 16/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.0958 - acc: 0.9723 - mean_absolute_error: 0.0083\n",
      "Epoch 17/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.0965 - acc: 0.9720 - mean_absolute_error: 0.0082\n",
      "Epoch 18/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.0899 - acc: 0.9743 - mean_absolute_error: 0.0077\n",
      "Epoch 19/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.0913 - acc: 0.9745 - mean_absolute_error: 0.0077\n",
      "Epoch 20/20\n",
      "52500/52500 [==============================] - 2s 39us/step - loss: 0.0890 - acc: 0.9749 - mean_absolute_error: 0.0075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x153886cb6a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train_cat,batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dense = model.predict(X_test)\n",
    "y_pred_dense = y_pred_dense.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification_report=\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9765    0.9823    0.9794      1694\n",
      "           1     0.9881    0.9819    0.9850      1939\n",
      "           2     0.9376    0.9697    0.9534      1752\n",
      "           3     0.9427    0.9560    0.9493      1771\n",
      "           4     0.9709    0.9466    0.9586      1724\n",
      "           5     0.9567    0.9361    0.9463      1580\n",
      "           6     0.9736    0.9780    0.9757      1769\n",
      "           7     0.9671    0.9639    0.9655      1802\n",
      "           8     0.9548    0.9481    0.9514      1715\n",
      "           9     0.9516    0.9538    0.9527      1754\n",
      "\n",
      "   micro avg     0.9621    0.9621    0.9621     17500\n",
      "   macro avg     0.9620    0.9616    0.9617     17500\n",
      "weighted avg     0.9622    0.9621    0.9621     17500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test_num, y_pred_dense, digits=4)\n",
    "print(f'\\nClassification_report=\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',  optimizer=sgd,  metrics=['accuracy', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "52500/52500 [==============================] - 7s 141us/step - loss: 0.2728 - acc: 0.9141 - mean_absolute_error: 0.0248\n",
      "Epoch 2/20\n",
      "52500/52500 [==============================] - 7s 131us/step - loss: 0.1092 - acc: 0.9678 - mean_absolute_error: 0.0099\n",
      "Epoch 3/20\n",
      "52500/52500 [==============================] - 7s 132us/step - loss: 0.0858 - acc: 0.9740 - mean_absolute_error: 0.0078\n",
      "Epoch 4/20\n",
      "52500/52500 [==============================] - 7s 133us/step - loss: 0.0730 - acc: 0.9775 - mean_absolute_error: 0.0066\n",
      "Epoch 5/20\n",
      "52500/52500 [==============================] - 7s 131us/step - loss: 0.0631 - acc: 0.9802 - mean_absolute_error: 0.0057\n",
      "Epoch 6/20\n",
      "52500/52500 [==============================] - 7s 131us/step - loss: 0.0577 - acc: 0.9815 - mean_absolute_error: 0.0052\n",
      "Epoch 7/20\n",
      "52500/52500 [==============================] - 7s 132us/step - loss: 0.0569 - acc: 0.9824 - mean_absolute_error: 0.0050\n",
      "Epoch 8/20\n",
      "52500/52500 [==============================] - 7s 134us/step - loss: 0.0490 - acc: 0.9847 - mean_absolute_error: 0.0044\n",
      "Epoch 9/20\n",
      "52500/52500 [==============================] - 7s 140us/step - loss: 0.0468 - acc: 0.9852 - mean_absolute_error: 0.0042\n",
      "Epoch 10/20\n",
      "52500/52500 [==============================] - 8s 144us/step - loss: 0.0393 - acc: 0.9876 - mean_absolute_error: 0.0035\n",
      "Epoch 11/20\n",
      "52500/52500 [==============================] - 7s 140us/step - loss: 0.0395 - acc: 0.9874 - mean_absolute_error: 0.0036\n",
      "Epoch 12/20\n",
      "52500/52500 [==============================] - 7s 141us/step - loss: 0.0372 - acc: 0.9882 - mean_absolute_error: 0.0033\n",
      "Epoch 13/20\n",
      "52500/52500 [==============================] - 8s 154us/step - loss: 0.0361 - acc: 0.9885 - mean_absolute_error: 0.0033\n",
      "Epoch 14/20\n",
      "52500/52500 [==============================] - 7s 134us/step - loss: 0.0346 - acc: 0.9889 - mean_absolute_error: 0.0030\n",
      "Epoch 15/20\n",
      "52500/52500 [==============================] - 7s 132us/step - loss: 0.0318 - acc: 0.9899 - mean_absolute_error: 0.0029\n",
      "Epoch 16/20\n",
      "52500/52500 [==============================] - 7s 133us/step - loss: 0.0290 - acc: 0.9913 - mean_absolute_error: 0.0026\n",
      "Epoch 17/20\n",
      "52500/52500 [==============================] - 7s 132us/step - loss: 0.0308 - acc: 0.9902 - mean_absolute_error: 0.0027\n",
      "Epoch 18/20\n",
      "52500/52500 [==============================] - 7s 132us/step - loss: 0.0285 - acc: 0.9911 - mean_absolute_error: 0.0025\n",
      "Epoch 19/20\n",
      "52500/52500 [==============================] - 7s 138us/step - loss: 0.0283 - acc: 0.9909 - mean_absolute_error: 0.0025\n",
      "Epoch 20/20\n",
      "52500/52500 [==============================] - 7s 133us/step - loss: 0.0258 - acc: 0.9915 - mean_absolute_error: 0.0023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15386daa4e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = X_train.shape[0]\n",
    "X_train_28 = X_train.reshape(N, 28, 28, 1)\n",
    "\n",
    "model.fit(X_train_28,y_train_cat,batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_test.shape[0]\n",
    "X_test_28 = X_test.reshape(N, 28, 28, 1)\n",
    "y_pred_cnn = model.predict(X_test_28)\n",
    "y_pred_cnn = y_pred_cnn.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification_report=\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9923    0.9941    0.9932      1694\n",
      "           1     0.9928    0.9974    0.9951      1939\n",
      "           2     0.9875    0.9909    0.9892      1752\n",
      "           3     0.9943    0.9904    0.9924      1771\n",
      "           4     0.9739    0.9948    0.9842      1724\n",
      "           5     0.9936    0.9848    0.9892      1580\n",
      "           6     0.9876    0.9915    0.9896      1769\n",
      "           7     0.9895    0.9900    0.9897      1802\n",
      "           8     0.9901    0.9913    0.9907      1715\n",
      "           9     0.9982    0.9732    0.9856      1754\n",
      "\n",
      "   micro avg     0.9899    0.9899    0.9899     17500\n",
      "   macro avg     0.9900    0.9898    0.9899     17500\n",
      "weighted avg     0.9900    0.9899    0.9899     17500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test_num, y_pred_cnn, digits=4)\n",
    "print(f'\\nClassification_report=\\n{report}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_custom",
   "language": "python",
   "name": "python3_custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
